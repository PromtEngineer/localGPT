{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ingest\n",
    "import run_localGPT\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "\n",
    "import requests\n",
    "from googlesearch import search\n",
    "import hashlib\n",
    "import os\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your GitHub personal access token\n",
    "TOKEN = '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'\n",
    "\n",
    "# Define the folder for storing database\n",
    "SOURCE_DIRECTORY = f\"tmp_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_url_as_html(url, save_path):\n",
    "    try:\n",
    "        # Send a GET request to the URL to fetch the content\n",
    "        response = requests.get(url, timeout=10)\n",
    "        \n",
    "        # Check if the request was successful (status code 200)\n",
    "        if response.status_code == 200:\n",
    "            # Save the content as an HTML file\n",
    "            with open(save_path, 'w', encoding='utf-8') as html_file:\n",
    "                html_file.write(response.text)\n",
    "            print(f\"HTML content saved as {save_path}\")\n",
    "        else:\n",
    "            print(f\"Failed to fetch the URL. Status code: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "def get_documents(dex_name, save_path=SOURCE_DIRECTORY):\n",
    "    # Extract links for liquidity model using googlesearch (only html files)\n",
    "    query = f'{dex_name} liquidity model'\n",
    "    search_results = search(query, num_results=1)\n",
    "    liquidity_model_link = list(search_results)\n",
    "\n",
    "    # create a folder for the dex liquidity model\n",
    "    os.makedirs(f'{save_path}/{dex_name}/liquidity_model', exist_ok=True)\n",
    "\n",
    "    # save liquidity model pages as html\n",
    "    for i, link in enumerate(liquidity_model_link):\n",
    "        try:\n",
    "            save_url_as_html(link, f'{save_path}/{dex_name}/liquidity_model/{hashlib.md5(link.encode()).hexdigest()}_{i+1}.html')\n",
    "        except:\n",
    "            print(\"Could not save the page.\")\n",
    "\n",
    "    # create a folder for the dex if it doesn't exist\n",
    "    os.makedirs(f'{save_path}/{dex_name}/license', exist_ok=True)\n",
    "    # Flag to track if a license has been found for this DEX\n",
    "    license_found = False\n",
    "    \n",
    "    # Make a GitHub API repository search request based on the DEX name\n",
    "    search_url = f'https://api.github.com/search/repositories?q={dex_name}&per_page=10'\n",
    "    headers = {'Authorization': f'token {TOKEN}'}\n",
    "    response = requests.get(search_url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        search_results = response.json()['items']\n",
    "\n",
    "        for repo in search_results:\n",
    "            # Check if a license file exists and retrieve the license text\n",
    "            license_url = f'https://api.github.com/repos/{repo[\"owner\"][\"login\"]}/{repo[\"name\"]}/license'\n",
    "            response = requests.get(license_url, headers=headers)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                license_data = response.json()\n",
    "                if 'content' in license_data:\n",
    "                    license_text = base64.b64decode(license_data['content']).decode('utf-8')\n",
    "                    # Save license text to a file in the dex folder in the license folder \n",
    "                    with open(f'{save_path}/{dex_name}/license/{repo[\"full_name\"].replace(\"/\", \"__\")}.txt', 'w') as f:\n",
    "                        f.write(license_text)\n",
    "                    # Set the flag to True to indicate that a license has been found\n",
    "                    license_found = True\n",
    "                    break  # Stop searching for licenses in other repositories for this DEX\n",
    "            else:\n",
    "                print(f'Failed to fetch license for {repo[\"full_name\"]}: {response.status_code}')\n",
    "    \n",
    "        # If no official license is found, create a txt file with the message\n",
    "        if not license_found:\n",
    "            with open(f'{save_path}/{dex_name}/license/no_license.txt', 'w') as f:\n",
    "                f.write(\"No official license found for this DEX.\")\n",
    "    else:\n",
    "        print(f'Failed to search for repositories related to {dex_name}: {response.status_code}')\n",
    "    \n",
    "    # return path to the dex folder\n",
    "    return f'{save_path}/{dex_name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Device type\n",
    "DEVICE_TYPE='cpu'\n",
    "# DEVICE_TYPE='cuda'\n",
    "\n",
    "PERSIST_DIRECTORY = f\"tmp_persist\"\n",
    "\n",
    "EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\" # Uses 0.2 GB of VRAM (Less accurate but fastest - only requires 150mb of vram)\n",
    "\n",
    "# Create embeddings\n",
    "EMBEDDINGS = HuggingFaceInstructEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL_NAME,\n",
    "    model_kwargs={\"device\": \"cpu\"},\n",
    "    )\n",
    "\n",
    "features = [\"liquidity_model\", \"license\"]\n",
    "\n",
    "# Get DEX name \n",
    "dex_name = ...\n",
    "\n",
    "# k, chunk_size, chunk_overlap\n",
    "k = ...\n",
    "cs = ...\n",
    "co = ...\n",
    "\n",
    "model_id = \"TheBloke/Llama-2-7b-Chat-GGUF\"\n",
    "model_basename = \"llama-2-7b-chat.Q4_K_M.gguf\"\n",
    "\n",
    "# Load model\n",
    "llm = run_localGPT.load_model(DEVICE_TYPE, model_id=model_id, model_basename=model_basename)\n",
    "\n",
    "for feature in features:\n",
    "    dir = f\"{dex_name}/{feature}\"\n",
    "    print(f\"Running for {dex_name} {feature} with k={k}, cs={cs}, co={co}\")\n",
    "\n",
    "    print(\"Ingesting...\")\n",
    "    source_directory = os.path.join(SOURCE_DIRECTORY, dir)\n",
    "\n",
    "    save_path = os.path.join(PERSIST_DIRECTORY, dir, os.path.basename(EMBEDDING_MODEL_NAME))\n",
    "    ingest.main(device_type=DEVICE_TYPE, embedding_model=EMBEDDINGS, chunk_size=cs, chunk_overlap=co,\\\n",
    "                source_directory=source_directory, save_path=save_path)\n",
    "\n",
    "    persist_directory = os.path.join(save_path, f'cs_{cs}_co_{co}')\n",
    "\n",
    "        # Getting the query from queries/feature.txt\n",
    "    with open(f\"queries/{feature}.txt\", \"r\") as f:\n",
    "        query = f.read()\n",
    "\n",
    "    # Replacing the DEX with the name of the DEX\n",
    "    query = query.replace(\"the DEX\", dex_name)\n",
    "\n",
    "    print(\"Running localGPT...\")\n",
    "    # Running localGPT\n",
    "    answer, docs = run_localGPT.main(DEVICE_TYPE, llm, k, persist_directory, query,\\\n",
    "                                    verbose=False, show_sources=False, promptTemplate_type=\"llama\")\n",
    "\n",
    "    # Showing the answer\n",
    "    print(\"Answer:\", answer)\n",
    "\n",
    "    # Showing the sources\n",
    "    print(\"----------------------------------SOURCE DOCUMENTS---------------------------\")\n",
    "    for document in docs:\n",
    "        print(\"\\n> \" + document.metadata[\"source\"] + \":\")\n",
    "        print(document.page_content)\n",
    "\n",
    "# When done, unload the model\n",
    "del llm\n",
    "torch.cuda.empty_cache()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_interaction(dex_name, k, co, cs, progress=gr.Progress()):\n",
    "    results = {}\n",
    "\n",
    "    # Check if all parameters are provided\n",
    "    if dex_name and k is not None and co is not None and cs is not None:\n",
    "        progress(0.2, desc=\"Scraping documents...\")\n",
    "        #time.sleep(1)\n",
    "        #dex_folder = get_documents(dex_name)\n",
    "        dex_folder = \"tmp_data/Uniswap v3\"\n",
    "\n",
    "        progress(0.4, desc=\"Loading embedding model...\")\n",
    "        #time.sleep(1)\n",
    "        embedding_model = HuggingFaceInstructEmbeddings(model_name=\"all-MiniLM-L6-v2\", model_kwargs={\"device\": \"cpu\"})\n",
    "\n",
    "        progress(0.6, desc=\"Loading LLM model...\")\n",
    "        #time.sleep(1)\n",
    "        llm = run_localGPT.load_model(device_type=\"cpu\", model_id=\"TheBloke/Llama-2-7b-Chat-GGUF\", model_basename=\"llama-2-7b-chat.Q4_K_M.gguf\")\n",
    "\n",
    "        # Define features to process\n",
    "        features = [\"liquidity_model\", \"license\"]\n",
    "        \n",
    "        for feature in features:\n",
    "            source_directory = f\"{dex_folder}/{feature}\"\n",
    "            #progress(0.5, desc=\"Ingesting documents...\")\n",
    "            #time.sleep(1)\n",
    "            save_path = f\"{source_directory}/{embedding_model.model_name}\"\n",
    "            #ingest.main(device_type=\"cpu\", embedding_model=embedding_model, chunk_size=cs, chunk_overlap=co,\n",
    "                        #source_directory=source_directory, save_path=save_path)\n",
    "\n",
    "            # Getting the query from queries/feature.txt\n",
    "            with open(f\"queries/{feature}.txt\", \"r\") as f:\n",
    "                query = f.read()\n",
    "                query = query.replace(\"the DEX\", dex_name)\n",
    "\n",
    "            # Convert k to an integer\n",
    "            k = int(k)\n",
    "\n",
    "            # Running localGPT\n",
    "            #progress(1, desc=\"Running localGPT...\")\n",
    "            #time.sleep(1)\n",
    "            answer, docs = run_localGPT.main(\"cpu\", llm, k, save_path, query, verbose=False, show_sources=False, promptTemplate_type=\"llama\")\n",
    "\n",
    "            # Store the results\n",
    "            results[feature] = {\"answer\": answer, \"sources\": [document.page_content for document in docs]}\n",
    "            print(len(docs))\n",
    "\n",
    "        # Unload the model and free up resources\n",
    "        #del llm\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7920\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7920/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def ui(dex, k, co, cs):\n",
    "    # Your existing code here\n",
    "    results = {\"answer\": {\"liquidity model\": \"LM1\", \"license\": \"MIT\"}, \"sources\": {\"liquidity model\": [\"source1\", \"source2\"], \"license\": [\"source1\", \"source2\"]}}\n",
    "    status = \"Process completed.\"\n",
    "    return status, results\n",
    "\n",
    "df = pd.DataFrame({\"Dex name\": [\"Uniswap v3\", \"SushiSwap\", \"PancakeSwap\"], \"Liquidity model\": [\"LM1\", \"LM2\", \"LM3\"], \"License\": [\"MIT\", \"Apache\", \"GPL\"]})\n",
    "\n",
    "\n",
    "with gr.Blocks(gr.themes.Default()) as demo:\n",
    "    # title\n",
    "    gr.Markdown(\"<h1 style='color: #4285F4; font-size: 36px;'>ðŸŒŸ DEX Navigator</h1>\")\n",
    "    gr.Markdown(\"<p style='font-size: 20px;'>An app that helps you find answers to questions about a DEX.</p>\")\n",
    "    with gr.Tab(\"Table\"):\n",
    "        table = gr.DataFrame(df)\n",
    "    with gr.Tab(\"Interaction\"):\n",
    "        with gr.Column():\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    dex = gr.Textbox(label=\"DEX Name\")\n",
    "                    gr.Slider(minimum=0, maximum=1, value=0, label=\"Temperature\", info=\"Choose between 0 and 1\")\n",
    "                    gr.Slider(minimum=0, maximum=1, value=0, label=\"Top P\", info=\"Choose between 0 and 1\")\n",
    "                with gr.Column(\"Ingesting Parameters\"):\n",
    "                    cs = gr.Number(label=\"Chunk Size\")\n",
    "                    co = gr.Number(label=\"Chunk Overlap\")\n",
    "                    k = gr.Number(label=\"Number of Chunks\", minimum=1, maximum=5, value=3, info=\"Choose between 1 and 5\")                \n",
    "            with gr.Column():\n",
    "                results = gr.JSON(label=\"Results\")\n",
    "        extract_button = gr.Button(\"Extract\")\n",
    "    extract_button.click(user_interaction, inputs=[dex, k, co, cs], outputs=[results])\n",
    "\n",
    "demo.queue(concurrency_count=20).launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
